subset
subset<- subset(captaincy,played>20,select=c("names","Y","played","won","lost"))
subset
subset<- subset(captaincy,lost<14,select=c("names","Y","played","won","lost"))
subset
v<-c(9,10,11,12)
v1<-c(13,14,15,16)
matrix
matrixa<-matrix(v,nrow = 4,ncol = 2,byrow = TRUE)
matrixa
matrixb<-matrix(v1,nrow = 4,ncol = 2,byrow = TRUE)
matrixa+matrixb
matrixa-matrixb
matrixa*matrixb
matrixa%*%matrixb
matrixa %*% matrixb
matrixa %*% matrixb
x<-1:4
y<-2:3
x+y
x<-c(17,14,4,5,13,12,10)
x[x>10]<-4
x
x<-c(17,14,4,5,13,12,10)
x[x>10]==4
x<-4
class(x)
x<-c(4,TRUE)
class(x)
y<-c(4,TRUE)
class(y)
x<-c(1,3,5)
y<-c(3,2,10)
cbind(x,y)
x<-list(2,"a","b",TRUE)
x[[1]]
class(x[[1]])
x<-1:
y<-2:3
y<-2:3
x+y
x<-1:4
x+y
x<_c(17,14,4,5,13,12,10)
x<-c(17,14,4,5,13,12,10)
x[x>10]==4
write.csv(hw,file = "hw1_data.csv")
write.csv(mq,file = "hw1_data.csv")
write.csv(CO2,file = "hw1_data.csv")
view(CO2)
view(CO2)
view(CO2)
write.csv(CO2,file = "hw1_data.csv")
View(CO2)
write.csv(mw,file = "hw1_data.csv")
mw<-data.frame("hw1_data.csv")
View(mw)
write.csv(mo,file = mw)
write.csv(mw,file = mw)
write.csv(mw,file = ""mw")
getwd()
write.csv(mw,file = "hw1_data.csv")
View(mw)
write.csv(te,file = "hw1_data.csv")
write.csv(te,file = "hw1_data.csv")
mo<-read.csv("hw1_data.csv",TRUE,sep=",")
print(mo)
numberOfRowsToSaveForTestSet <- 100
justWeather <- function()
{
data(day1)
weather <- day1[, c(3, 9:13)] # season, month, + variables we want to predict
weather[,7] <- weather$season == 1
weather[,8] <- weather$season == 2
weather[,9] <- weather$season == 3 # if all false, season == 4
weather[,10] <- day1[,"mnth"]**2
weather[,11] <- weather$weathersit == 1
weather[,12] <- weather$weathersit == 2 # if both false, weathersit == 3
colnames(weather)[7:12] <- c("seasonIsWinter", "seasonIsSpring", "seasonIsSummer", "monthSquared", "weathersitIsOne", "weathersitIsTwo")
weather
}
install.packages("forecast")
names(csv)
names(csv)
names(csv)
names(data)
range(data$DATE)
library(xts)
historical = ts_regular(historical)
historical = ts_regular(historical)
historical = na.fill(historical, "extend")
historical = ts_regular(historical)
csv = read.csv("data.csv", as.is=T)
install.packages("xts")
install.packages("tsbox")
install.packages("forecast")
csv = read.csv("data.csv", as.is=T)
csv = read.csv("data.csv")
csv <- read.csv("data.csv")
setwd("C:/Users/saura/Desktop/BE Project")
csv <- read.csv("data.csv")
csv <- read.csv("data.csv")
names(csv)
range(csv$DATE)
library(xts)
library(tsbox)
historical = xts(csv[,c("TMAX","TMIN","PRCP")], order.by=as.Date(csv$DATE))
csv <- read.csv("data.csv")
names(csv)
range(csv$DATE)
library(xts)
library(tsbox)
historical = xts(csv[,c("TMAX","TMIN","PRCP")], order.by=as.Date(csv$DATE))
csv <- read.csv("data.csv")
names(csv)
range(csv$DATE)
library(xts)
library(tsbox)
historical = xts(csv[,c("TMAX","TMIN","PRCP")], order.by=as.Date(csv$DATE))
install.packages("xts")
install.packages("tsbox")
install.packages("forecast")
csv <- read.csv("data.csv")
names(csv)
range(csv$DATE)
library(xts)
library(tsbox)
historical <- xts(csv[,c("TMAX","TMIN","PRCP")], order.by=as.Date(csv$DATE))
#install.packages("xts")
#install.packages("tsbox")
#install.packages("forecast")
csv <- read.csv("data.csv")
names(csv)
range(csv$DATE)
library(xts)
library(tsbox)
#historical <- xts(csv[,c("TMAX","TMIN","PRCP")], order.by=as.Date(csv$DATE))
historical <- ts_regular(historical)
#install.packages("xts")
#install.packages("tsbox")
#install.packages("forecast")
csv <- read.csv("data.csv")
names(csv)
range(csv$DATE)
library(xts)
library(tsbox)
historical <- xts(csv[,c("TMAX","TMIN","PRCP")])
#install.packages("xts")
#install.packages("tsbox")
#install.packages("forecast")
csv <- read.csv("data.csv")
names(csv)
range(csv$DATE)
library(xts)
library(tsbox)
historical <- xts(csv[,c("TMAX","TMIN","PRCP")], order.by=as.Date(csv$DATE))
View(justWeather)
View(justWeather)
#install.packages("devtools")
#install.packages("tidyverse")
library(tidyverse)
library(devtools)
#install_github("matloff/regtools")
library(regtools)
k <- 1
numberOfRowsToSaveForTestSet <- 100
justWeather <- function()
{
data(day1)
weather <- day1[, c(3, 9:13)] # season, month, + variables we want to predict
weather[,7] <- weather$season == 1
weather[,8] <- weather$season == 2
weather[,9] <- weather$season == 3 # if all false, season == 4
weather[,10] <- day1[,"mnth"]**2
weather[,11] <- weather$weathersit == 1
weather[,12] <- weather$weathersit == 2 # if both false, weathersit == 3
colnames(weather)[7:12] <- c("seasonIsWinter", "seasonIsSpring", "seasonIsSummer", "monthSquared", "weathersitIsOne", "weathersitIsTwo")
weather
}
day1Expanded <- justWeather()
# Take careful note that this drops the first k rows! You should drop the first k rows of the original matrix afterwards!
makeXMatrix <- function(columnsToUseNames) # e.g. makeXMatrix(c(4, 5, 8))
{
originalColnames <- colnames(day1Expanded)
xmatrix <- day1Expanded
for (i in 1:k) {
for (j in 1:length(columnsToUseNames))
{
newColumnIndex <- ncol(day1Expanded) + (i-1)*length(columnsToUseNames) + j # we have already added (i-1)*length(columnsToUse) columns
xmatrix <- cbind(xmatrix, 0)
for (h in (k+1):nrow(day1Expanded)) # we will throw away the first k rows, because they don't have datapoints from k days prior
{
iDaysAgo <- h - i
xmatrix[h, newColumnIndex] <- day1Expanded[iDaysAgo, columnsToUseNames[j]]
}
colnames(xmatrix)[newColumnIndex] <- paste(columnsToUseNames[j], i, "DaysAgo", sep="")
}
}
xmatrix <- xmatrix[, !(names(xmatrix) %in% originalColnames)] # We only keep predictors from days ago, getting rid of originals
xmatrix <- tail(xmatrix, -1*k) # drop first k rows
as.matrix(xmatrix)
}
findValueError <- function(testXMatrix, testResponseVar, modelCoefficients)
{
predictions <- cbind(1, testXMatrix) %*% modelCoefficients
avgSquaredError <- mean(abs(predictions - testResponseVar))
avgSquaredError
}
findWeathersitError <- function(testXMatrices, testResponseVar, models)
{
likelihoodsOfEachWeathersit <- list()
for (i in 1:3) {
testXMatrix <- cbind(1, testXMatrices[[i]])
likelihoodsOfEachWeathersit[[i]] <- testXMatrix %*% models[[i]]$coefficients
}
weathersitProbabilities <- cbind(likelihoodsOfEachWeathersit[[1]], likelihoodsOfEachWeathersit[[2]],likelihoodsOfEachWeathersit[[3]])
chooseBestWeathersit <- function(probabilityWeathersitIs)
{
if (probabilityWeathersitIs[1] >= probabilityWeathersitIs[2] && probabilityWeathersitIs[1] >= probabilityWeathersitIs[3])
{
return(1)
}
else if (probabilityWeathersitIs[2] >= probabilityWeathersitIs[3])
{
return(2)
}
return(3)
}
bestWeathersitPrediction <- apply(weathersitProbabilities, 1, chooseBestWeathersit)
return(mean(testResponseVar == bestWeathersitPrediction))
}
modelValue <- function(responseVarColName, columnNamesOfPredictors)
{
xMatrix <- makeXMatrix(columnNamesOfPredictors)
day1ExpandedChopped <- tail(day1Expanded, -1*k) # get rid of first k rows, since they are not part of the xmatrix
trainingSet <- head(day1ExpandedChopped, -1*numberOfRowsToSaveForTestSet)
testSet <- tail(day1ExpandedChopped, numberOfRowsToSaveForTestSet)
trainingXMatrix <- head(xMatrix, -1*numberOfRowsToSaveForTestSet) # this is time-series data. By choosing the last rows as our test set, we avoid letting the test set influence training
testXMatrix <- tail(xMatrix, numberOfRowsToSaveForTestSet)
trainingResponseVar <- trainingSet[,responseVarColName]
model <- lm(trainingResponseVar ~ trainingXMatrix)
betaHat <- model$coefficients
print(paste("AdjRSquared:", summary(model)$adj.r.squared))
print(paste("Average Error:", findValueError(testXMatrix, testSet[, responseVarColName], betaHat)))
#model
}
# in order, colnamesOfPredictors should be a list of vectors of predictors that: weathersit == 1, weathersit == 2, weathersit == 3, and weathersit == 4
# syntax: list(c("predictor1ForWeathersitBeing1", "predictor2ForWeathersitBeing1"), ..., c("predictor1ForWeathersitBeing4"))
modelWeathersit <- function(colnamesOfPredictors) # weathersit == 4 has never happened in the data; we won't even consider it as a possibility when predicting the next day's weathersit
{
xMatrices <- list()
trainingXMatrices <- list()
testXMatrices <- list()
for (i in 1:3)
{
xMatrix <- makeXMatrix(colnamesOfPredictors[[i]])
xMatrices[[i]] <- xMatrix
trainingXMatrices[[i]] <- head(xMatrix, -1*numberOfRowsToSaveForTestSet) # this is time-series data. By choosing the last rows as our test set, we avoid letting the test set influence training
testXMatrices[[i]] <- tail(xMatrix, numberOfRowsToSaveForTestSet)
}
day1ExpandedChopped <- tail(day1Expanded, -1*k)
trainingSet <- head(day1ExpandedChopped, -1*numberOfRowsToSaveForTestSet)
testSet <- tail(day1ExpandedChopped, numberOfRowsToSaveForTestSet)
isWeathersitThreeNumeric <- function(weatherRow)
{
as.numeric(weatherRow["weathersit"] == 3)
}
trainingWeathersitIsThree <- apply(trainingSet, 1, isWeathersitThreeNumeric)
trainingResponseVars <- list(trainingSet[, "weathersitIsOne"], trainingSet[, "weathersitIsTwo"], trainingWeathersitIsThree)
models <- list()
for (i in 1:3)
{
models[[i]] <- glm(trainingResponseVars[[i]] ~ trainingXMatrices[[i]], family=binomial)
#print(summary(models[[i]]))
}
testResponseVars <- testSet[, "weathersit"]
findWeathersitError(testXMatrices, testResponseVars, models)
}
modelAll <- function()
{
print("windspeed:")
modelValue("windspeed", c("windspeed"))#, "weathersitIsOne", "weathersitIsTwo", "monthSquared"))
print("temp:")
modelValue("temp", c("temp"))
print("atemp:")
modelValue("atemp", c("atemp"))
print("hum:")
modelValue("hum", c("hum"))
weathersitPredictors <-
list(
c("weathersitIsOne"),#, "weathersitIsTwo"),
c("weathersitIsOne"),#, "weathersitIsTwo"),
c("weathersitIsOne", "weathersitIsTwo")
)
paste("Proportion of time we predict weathersit correctly:", modelWeathersit(weathersitPredictors))
}
csv <- read.csv("data.csv")
names(csv)
range(csv$DATE)
library(xts)
library(tsbox)
historical <- xts(csv[,c("TMAX","TMIN","PRCP")], order.by=as.Date(csv$DATE))
csv <- read.csv("data.csv")
names(csv)
range(csv$DATE)
library(xts)
library(tsbox)
historical <- xts(csv[,c("TMAX","TMIN","PRCP")], order.by=as.Date(csv$DATE))
setwd("C:/Users/saura/Desktop/BE Project")
setwd("C:/Users/saura/Desktop/customer-segmentation-dataset")
customer_data=read.csv("C:\Users\saura\Desktop\customer-segmentation-dataset/Mall_Customers.csv")
customer_data=read.csv("Mall_Customers.csv")
str(customer_data)
names(customer_data)
customer_data=read.csv("Mall_Customers.csv")
str(customer_data)
names(customer_data)
head(customer_data)
summary(customer_data$Age)
sd(customer_data$Age)
summary(customer_data$Annual.Income..k..)
sd(customer_data$Annual.Income..k..)
summary(customer_data$Age)
sd(customer_data$Spending.Score..1.100.)
a=table(customer_data$Gender)
barplot(a,main="Using BarPlot to display Gender Comparision",
ylab="Count",
xlab="Gender",
col=rainbow(2),
legend=rownames(a))
customer_data=read.csv("Mall_Customers.csv")
str(customer_data)
names(customer_data)
head(customer_data)
summary(customer_data$Age)
sd(customer_data$Age)
summary(customer_data$Annual.Income..k..)
sd(customer_data$Annual.Income..k..)
summary(customer_data$Age)
sd(customer_data$Spending.Score..1.100.)
a=table(customer_data$Gender)
barplot(a,main="Using BarPlot to display Gender Comparision",
ylab="Count",
xlab="Gender",
col=rainbow(2),
legend=rownames(a))
a=table(customer_data$Gender)
barplot(a,main="Using BarPlot to display Gender Comparision",
ylab="Count",
xlab="Gender",
col=rainbow(2),
legend=rownames(a))
pct=round(a/sum(a)*100)
lbs=paste(c("Female","Male")," ",pct,"%",sep=" ")
library(plotrix)
summary(customer_data$Age)
pie3D(a,labels=lbs,
main="Pie Chart Depicting Ratio of Female and Male")
hist(customer_data$Age,
col="blue",
main="Histogram to Show Count of Age Class",
xlab="Age Class",
ylab="Frequency",
labels=TRUE)
boxplot(customer_data$Age,
col="ff0066",
main="Boxplot for Descriptive Analysis of Age")
summary(customer_data$Annual.Income..k..)
hist(customer_data$Annual.Income..k..,
col="#660033",
main="Histogram for Annual Income",
xlab="Annual Income Class",
ylab="Frequency",
labels=TRUE)
customer_data=read.csv("Mall_Customers.csv")
str(customer_data)
str(customer_data)
names(customer_data)
head(customer_data)
summary(customer_data$Age)
sd(customer_data$Age)
summary(customer_data$Annual.Income..k..)
sd(customer_data$Annual.Income..k..)
summary(customer_data$Age)
sd(customer_data$Spending.Score..1.100.)
a=table(customer_data$Gender)
barplot(a,main="Using BarPlot to display Gender Comparision",
ylab="Count",
xlab="Gender",
col=rainbow(2),
legend=rownames(a))
pct=round(a/sum(a)*100)
lbs=paste(c("Female","Male")," ",pct,"%",sep=" ")
library(plotrix)
pct=round(a/sum(a)*100)
lbs=paste(c("Female","Male")," ",pct,"%",sep=" ")
library(plotrix)
install.packages("plotrix")
library(plotrix)
pie3D(a,labels=lbs,
main="Pie Chart Depicting Ratio of Female and Male")
summary(customer_data$Age)
hist(customer_data$Age,
col="blue",
main="Histogram to Show Count of Age Class",
xlab="Age Class",
ylab="Frequency",
labels=TRUE)
boxplot(customer_data$Age,
col="ff0066",
main="Boxplot for Descriptive Analysis of Age")
boxplot(customer_data$Age,
col="#ff0066",
main="Boxplot for Descriptive Analysis of Age")
summary(customer_data$Annual.Income..k..)
hist(customer_data$Annual.Income..k..,
col="#660033",
main="Histogram for Annual Income",
xlab="Annual Income Class",
ylab="Frequency",
labels=TRUE)
plot(density(customer_data$Annual.Income..k..),
col="yellow",
main="Density Plot for Annual Income",
xlab="Annual Income Class",
ylab="Density")
polygon(density(customer_data$Annual.Income..k..),
col="#ccff66")
summary(customer_data$Spending.Score..1.100.)
boxplot(customer_data$Spending.Score..1.100.,
horizontal=TRUE,
col="#990000",
main="BoxPlot for Descriptive Analysis of Spending Score")
hist(customer_data$Spending.Score..1.100.,
main="HistoGram for Spending Score",
xlab="Spending Score Class",
ylab="Frequency",
col="#6600cc",
labels=TRUE)
library(purrr)
set.seed(123)
# function to calculate total intra-cluster sum of square
iss <- function(k) {
kmeans(customer_data[,3:5],k,iter.max=100,nstart=100,algorithm="Lloyd" )$tot.withinss
}
k.values <- 1:10
iss_values <- map_dbl(k.values, iss)
plot(k.values, iss_values,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total intra-clusters sum of squares")
library(cluster)
library(gridExtra)
install.packages("gridExtra")
library(cluster)
library(gridExtra)
library(grid)
k2<-kmeans(customer_data[,3:5],2,iter.max=100,nstart=50,algorithm="Lloyd")
s2<-plot(silhouette(k2$cluster,dist(customer_data[,3:5],"euclidean")))
k5<-kmeans(customer_data[,3:5],5,iter.max=100,nstart=50,algorithm="Lloyd")
s5<-plot(silhouette(k5$cluster,dist(customer_data[,3:5],"euclidean")))
k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
s6<-plot(silhouette(k6$cluster,dist(customer_data[,3:5],"euclidean")))
k7<-kmeans(customer_data[,3:5],7,iter.max=100,nstart=50,algorithm="Lloyd")
s7<-plot(silhouette(k7$cluster,dist(customer_data[,3:5],"euclidean")))
k8<-kmeans(customer_data[,3:5],8,iter.max=100,nstart=50,algorithm="Lloyd")
s8<-plot(silhouette(k8$cluster,dist(customer_data[,3:5],"euclidean")))
k9<-kmeans(customer_data[,3:5],9,iter.max=100,nstart=50,algorithm="Lloyd")
s9<-plot(silhouette(k9$cluster,dist(customer_data[,3:5],"euclidean")))
k10<-kmeans(customer_data[,3:5],10,iter.max=100,nstart=50,algorithm="Lloyd")
s10<-plot(silhouette(k10$cluster,dist(customer_data[,3:5],"euclidean")))
library(NbClust)
install.packages("NbClust")
fviz_nbclust(customer_data[,3:5], kmeans, method = "silhouette")
library(factoextra)
install.packages("factoextra")
fviz_nbclust(customer_data[,3:5], kmeans, method = "silhouette")
library(NbClust)
library(factoextra)
fviz_nbclust(customer_data[,3:5], kmeans, method = "silhouette")
set.seed(125)
stat_gap <- clusGap(customer_data[,3:5], FUN = kmeans, nstart = 25,
K.max = 10, B = 50)
fviz_gap_stat(stat_gap)
plot(pcclust$x[,1:2], col =kCols(digCluster),pch =19,xlab ="K-means",ylab="classes")
ggplot(customer_data, aes(x =Spending.Score..1.100., y =Age)) +
geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
scale_color_discrete(name=" ",
breaks=c("1", "2", "3", "4", "5","6"),
labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
kCols=function(vec){cols=rainbow (length (unique (vec)))
return (cols[as.numeric(as.factor(vec))])}
digCluster<-k6$cluster; dignm<-as.character(digCluster); # K-means clusters
k2<-kmeans(customer_data[,3:5],2,iter.max=100,nstart=50,algorithm="Lloyd")
s2<-plot(silhouette(k2$cluster,dist(customer_data[,3:5],"euclidean")))
k3<-kmeans(customer_data[,3:5],3,iter.max=100,nstart=50,algorithm="Lloyd")
s3<-plot(silhouette(k3$cluster,dist(customer_data[,3:5],"euclidean")))
k4<-kmeans(customer_data[,3:5],4,iter.max=100,nstart=50,algorithm="Lloyd")
s4<-plot(silhouette(k4$cluster,dist(customer_data[,3:5],"euclidean")))
k5<-kmeans(customer_data[,3:5],5,iter.max=100,nstart=50,algorithm="Lloyd")
s5<-plot(silhouette(k5$cluster,dist(customer_data[,3:5],"euclidean")))
k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
s6<-plot(silhouette(k6$cluster,dist(customer_data[,3:5],"euclidean")))
k7<-kmeans(customer_data[,3:5],7,iter.max=100,nstart=50,algorithm="Lloyd")
s7<-plot(silhouette(k7$cluster,dist(customer_data[,3:5],"euclidean")))
k8<-kmeans(customer_data[,3:5],8,iter.max=100,nstart=50,algorithm="Lloyd")
s8<-plot(silhouette(k8$cluster,dist(customer_data[,3:5],"euclidean")))
k9<-kmeans(customer_data[,3:5],9,iter.max=100,nstart=50,algorithm="Lloyd")
s9<-plot(silhouette(k9$cluster,dist(customer_data[,3:5],"euclidean")))
k10<-kmeans(customer_data[,3:5],10,iter.max=100,nstart=50,algorithm="Lloyd")
s10<-plot(silhouette(k10$cluster,dist(customer_data[,3:5],"euclidean")))
